
# 크롤링

1. 웹 사이트에서 내용을 가져오는 방법
    - 웹 사이트에 존재하는 거의 대부분의 내용을 가져올 수 있다
    - 텍스트, 영상, 그림, 음성 등등...
    - 다운로드, 배포시 저작권 문제에 주의
    - 과도한 트래픽을 유발할 가능성
2. 오픈 API를 통해서 가져오는 방법
    - 가장 권장하는 방법
    - 수집을 허용하기 때문에 API가 제공





## 소켓 프로그래밍을 이용한 HTTP 통신

    - 크롤링이란 HTTP를 이용해서 원하는 자원을 가져오는 방법
    - 일반적으로는 웹 페이지(html)를 주로 요청
    - 여러가지 형태의 파일을 HTTP 통신을 통해 가져올 수 있음





## 라이브러리를 이용한 HTTP 통신

    - 소켓 프로그래밍을 하지 않고도 HTTP 통신 가능
    - urllib, requests





### urllib 을 이용한 HTTP 통신

    - 파이썬 기본 패키지
    - 따로 설치 필요 x
    
    import urllib.request
    
    url = 'http://info.cern.ch'
    
    request = urllib.request.Request(url)
    response = urllib.request.urlopen( request )
    print( response.read().decode() )



### urllib을 통한 파일 저장

- urllib은 리퀘스트 객체를 생성할 때, 헤더값을 직접 정의

- fake_agent를 이용해서 `user-agent` 헤더의 값을 크롬 크라우저의 값과 동일하게 설정

- ```python
  prompt> pip install fake_useragent
  
  import urllib.request
  from fake_useragent import UserAgent
  
  # 파이썬이 아닌 웹 브라우저를 통해서 요청하는 것 처럼 보이기 위해
  # 헤더값을 직접 설정해준다
  agent = UserAgent()
  header = {'User-Agent': agent.chrome}
  
  url = 'https://image.zdnet.co.kr/2022/01/01/e601fd8d72cc33ca75cd9d41d3315684.jpg'
  request = urllib.request.Request( url, headers=header )
  response = urllib.request.urlopen( request )
  print(response.read())
  ```



### urllib.request.urlretrieve를 통한 파일 저장

- ```python
  import urllib.request
  from fake_useragent import UserAgent
  
  # UserAgent 객체 생성
  agent = UserAgent()
  
  # urllib은 urlretrieve함수를 이용해서 한 번에 파일로 저장
  url = 'https://image.zdnet.co.kr/2022/01/01/e601fd8d72cc33ca75cd9d41d3315684.jpg'
  
  # 파일을 저장할 경로
  path = 'web/data/download.jpg'
  
  # opener 객체를 생성해서 헤더를 먼저 수정
  opener = urllib.request.build_opener()
  opener.addheaders = [('User-Agent', agent.chrome)]
  urllib.request.install_opener(opener)
  
  # 이제는 urlretrieve를 이용해서 다운로드 한 파일을 바로 생성할 수 있음
  urllib.request.urlretrieve( url, path )
  ```



### request를 통한 HTTP 통신

- 설치가 필요한 패키지

- ```python
  prompt> pip install requests
  
  import requests
  
  url = 'http://info.cern.ch'
  
  response = requests.get(url)
  print( response.text )
  ```



### request를 통한 파일 저장

- ```python
  import requests
  from fake_useragent import UserAgent
  
  agent = UserAgent()
  header = {'User-Agent':agent.chrome}
  
  url = 'https://image.zdnet.co.kr/2022/01/01/e601fd8d72cc33ca75cd9d41d3315684.jpg'
  
  response = requests.get(url, headers=header)
  print( response.content )
  ```

- 파일로 저장하는 경우에는 `바이너리`형태로 파일 객체를 생성하고 저장

- ```python
  import requests
  from fake_useragent import UserAgent
  
  agent = UserAgent()
  header = {'User-Agent':agent.chrome}
  
  url = 'https://image.zdnet.co.kr/2022/01/01/e601fd8d72cc33ca75cd9d41d3315684.jpg'
  
  response = requests.get(url, headers=header)
  
  with open('web/data/download4.jpg', 'wb') as file:
    file.write( response.content )
  ```



# 스크래핑

- 웹 페이지에서 내가 원하는 내용만 가져오는 기능
- 크롤링된 웹 페이지에서 원하는 내용을 찾는 것 (파싱)



## 영화리뷰 페이지 크롤링

- 크롤링 된 HTML 내에 내가 원하는 내용이 포함

- HTML로 부터 원하는 요소를 찾는 것 (파싱)

- ```python
  import requests
  
  url = 'https://movie.naver.com/movie/point/af/list.naver'
  response = requests.get(url)
  html = response.text
  ```



## Beautiful Soup

- CSS 셀렉터를 이용해서 원하는 데이터를 검색할 수 있다

- 설치하기

- ```python
  prompt> pip install bs4
  
  # BeautifulSoup 패키지를 임포트
  import bs4
  
  # 수집된 HTML은 일반 텍스트 이기 때문에
  # bs4 객체로 변환
  review = bs4.BeautifulSoup(html)
  type(review)
  bs4.BeautifulSoup
  
  ```



## BS4를 이용한 파싱

- find, find_all, select, select_one



### find

- ```python
  # 태그명 검색
  # 일치하는 태그가 여러개인 경우에는 제일 처음 일치하는 태그의 요소를 반환
  element = review.find('a')
  print( type(element) )
  print(element)
  
  <class 'bs4.element.Tag'>
  <a name="gnb_top"></a>
  
  # 일치하는 모든 요소에 대해서 리스트 형태로 반환
  elements = review.find_all('a')
  print(type(elements))
  print(elements)
  
  # 단일 속성인 경우 
  td_element = review.find('td', class_='title')
  td_element
  
  # 여러개의 속성을 이용해서 검색하고자 하는 경우
  # 딕셔너리 형태로 검색하고자 하는 속성을 추가해준다
  
  attrs = {
    'class':'title'
  }
  td_element = review.find('td', attrs=attrs)
  td_element
  ```



### select

- select는 find_all과 동일한 기능
- select_one은 find와 동일한 기능
- CSS Selector를 이용해서 요소를 선택

- ```python
  import requests
  import bs4
  
  url = 'https://movie.naver.com/movie/point/af/list.naver'
  response = requests.get(url)
  html = response.text
  review = bs4.BeautifulSoup(html)
  
  review.select_one('td.title')
  review.select('td.title')
  ```



















